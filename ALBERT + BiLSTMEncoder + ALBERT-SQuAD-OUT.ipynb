{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT + BiLSTM Encoder + ALBERT-SQuAD-OUT\n",
    "\n",
    "Implementaion of ALBERT is taken from Hugging Face library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import json\n",
    "import pandas as pd\n",
    "from squad import Squad\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AlbertConfig,\n",
    "    AlbertModel,\n",
    "    AlbertTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
    "from transformers.data.metrics.squad_metrics import compute_predictions_logits, get_final_text\n",
    "from evaluate_answers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"\"\n",
    "do_lower_case = True\n",
    "\n",
    "# Tokenizer for ALBERT's input format\n",
    "tokenizer_class = AlbertTokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    \"albert-base-v2\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and test dataframes\n",
    "train_sq = Squad(\"../data/train-v2.0.json\")\n",
    "test_sq = Squad(\"../data/dev-v2.0.json\")\n",
    "train_df = train_sq.get_dataframe()\n",
    "test_df = test_sq.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset(train_df, tokenizer):\n",
    "    \"\"\"\n",
    "    Create dataset from DataFrame\n",
    "    \n",
    "    returns: \n",
    "        dataset - pytorch dataset of training data features\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for i, question in enumerate(train_df['content']):\n",
    "        example = SquadExample(\n",
    "            qas_id=str(i),\n",
    "            question_text=question,\n",
    "            context_text=train_df['context'][i],\n",
    "            answer_text=train_df['answer'][i],\n",
    "            start_position_character=train_df['answer_start'][i],\n",
    "            title=\"Train\",\n",
    "            is_impossible=False,\n",
    "            answers=None,\n",
    "        )\n",
    "        examples.append(example)\n",
    "    \n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=128,\n",
    "        max_query_length=64,\n",
    "        is_training=True,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=32,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return dataset, features, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 130319/130319 [01:00<00:00, 2161.13it/s]\n",
      "add example index and unique id: 100%|██████████| 130319/130319 [00:00<00:00, 851733.63it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset, _, _ = create_train_dataset(train_df, tokenizer)\n",
    "\n",
    "\n",
    "train_sampler = SequentialSampler(dataset)\n",
    "train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertBiLSTM4QA(nn.Module):\n",
    "    def __init__(self, freeze_albert = True):\n",
    "        super(AlbertBiLSTM4QA, self).__init__()\n",
    "        \n",
    "        # create model's config\n",
    "        config_class, model_class = (AlbertConfig, AlbertModel)\n",
    "        config = config_class.from_pretrained(\"albert-base-v2\")\n",
    "        config.output_hidden_states=True\n",
    "        self.backbone = model_class.from_pretrained(\"albert-base-v2\", config=config)\n",
    "        \n",
    "        # freeze ALBERT layers if freeze_albert is True\n",
    "        if freeze_albert:\n",
    "            for param in self.backbone.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.backbone.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.backbone.pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.backbone.pooler_activation.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.encoder = nn.LSTM(768, 384, num_layers=3, dropout=0.2, bidirectional=True)\n",
    "        \n",
    "        self.QA = nn.Sequential(\n",
    "            nn.Linear(768,2)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, batch, device='cpu'):\n",
    "        # inference through ALBERT\n",
    "        self.backbone.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            features, _, _ = self.backbone(**inputs)\n",
    "        \n",
    "        # permute features dimensions to satisfy BiLSTM's input (seq_inp, batch_size, hidden_state)\n",
    "        features = features.permute(1,0,2)\n",
    "        x, _ = self.encoder(features)\n",
    "        # permute back\n",
    "        x = x.permute(1,0,2)\n",
    "        logits = self.QA(x)\n",
    "        \n",
    "        # get start and end logits also calculate loss\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).squeeze(1)\n",
    "        end_logits = end_logits.squeeze(-1).squeeze(1)\n",
    "        \n",
    "        start_positions = batch[3]\n",
    "        end_positions = batch[4]\n",
    "        \n",
    "        \n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss)/2\n",
    "        return total_loss, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelA = AlbertBiLSTM4QA(freeze_albert=True).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(modelA.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer):\n",
    "    model.zero_grad()\n",
    "    f = open(\"logs.txt\", \"w\")\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            model_loss ,start_logits, end_logits = model(batch,device=device)\n",
    "            loss += model_loss.item()          \n",
    "            model_loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            if idx % 100 == 0:\n",
    "                start_pred = torch.argmax(start_logits, dim=1).cpu()\n",
    "                end_pred = torch.argmax(end_logits, dim=1).cpu()\n",
    "                pair_accuracy = ((start_pred==batch[3])*(end_pred==batch[4])).sum().float() / len(batch[3])\n",
    "                start_accuracy = (start_pred==batch[3]).sum().float() / len(batch[3])\n",
    "                end_accuracy = (end_pred==batch[4]).sum().float() / len(batch[4])\n",
    "                string = f\"[{idx+1}/{len(train_dataloader)}]Epoch: {epoch+1}/{epochs} Loss: {model_loss.item()} Pair Accuracy: {pair_accuracy} Start Accuracy: {start_accuracy} End Accuracy: {end_accuracy}\"\n",
    "                print(string)\n",
    "                f.write(string)\n",
    "                torch.save(model.state_dict(), \"AlbertBiLSTM.pth\")\n",
    "    f.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(modelA, 2, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelA.load_state_dict(torch.load(\"AlbertBiLSTM.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 11873/11873 [00:04<00:00, 2755.04it/s]\n",
      "add example index and unique id: 100%|██████████| 11873/11873 [00:00<00:00, 445632.37it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset, test_features, test_examples = create_train_dataset(test_df, tokenizer)\n",
    "\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def predict(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        to_return = []\n",
    "        for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "            _ ,start_logits, end_logits = model(batch,device=device)      \n",
    "            \n",
    "            # Predict start and end\n",
    "            start_pred = torch.argmax(start_logits, dim=1).cpu()\n",
    "            end_pred = torch.argmax(end_logits, dim=1).cpu()\n",
    "            \n",
    "            for start, end in zip(start_pred, end_pred):\n",
    "                to_return.append((start.item(), end.item()))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(results):\n",
    "    to_return = []\n",
    "    for res, feat, example in zip(results, test_features, test_examples):\n",
    "        if res[0] == 0 and res[1] == 0:\n",
    "            to_return.append('')\n",
    "        else:\n",
    "            #  Clamp to min max start and end\n",
    "            tok_tokens = feat.tokens[res[0] : (res[1] + 1)]\n",
    "            if res[0] < min(feat.token_to_orig_map):\n",
    "                start = min(feat.token_to_orig_map)\n",
    "            elif res[0] > max(feat.token_to_orig_map):\n",
    "                start = max(feat.token_to_orig_map)\n",
    "            else:\n",
    "                start = res[0]\n",
    "                \n",
    "            if res[1] < min(feat.token_to_orig_map):\n",
    "                end = min(feat.token_to_orig_map)\n",
    "            elif res[1] > max(feat.token_to_orig_map):\n",
    "                end = max(feat.token_to_orig_map)\n",
    "            else:\n",
    "                end = res[1]\n",
    "            \n",
    "            # Convert to predicted text from albert tokenizer tokens\n",
    "            orig_doc_start = feat.token_to_orig_map[start]\n",
    "            orig_doc_end = feat.token_to_orig_map[end]\n",
    "            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
    "            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
    "\n",
    "            tok_text = tok_text.strip()\n",
    "            tok_text = \" \".join(tok_text.split())\n",
    "            orig_text = \" \".join(orig_tokens)\n",
    "            final_text = get_final_text(tok_text, orig_text, True, True)\n",
    "\n",
    "            to_return.append(final_text)\n",
    "    \n",
    "    answers = {}\n",
    "    for text,row in zip(to_return, test_df.loc):\n",
    "        answers[row.id] = text\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131f2ffb5927457e86779203f9cae4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=384.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_preds = predict(modelA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate_preds(my_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 17.05550408489851,\n",
      "  \"f1\": 18.155467079522154,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 0.47233468286099867,\n",
      "  \"HasAns_f1\": 2.675415086903988,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 33.59125315391085,\n",
      "  \"NoAns_f1\": 33.59125315391085,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# For the more representetive results we have taken script that squad owner's have written to check predictions\n",
    "dataset = test_sq.data\n",
    "preds = res\n",
    "na_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "qid_to_has_ans = make_qid_to_has_ans(dataset) \n",
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
    "                                      1.0)\n",
    "f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
    "                                   1.0)\n",
    "out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "if has_ans_qids:\n",
    "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
    "if no_ans_qids:\n",
    "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
    "print(json.dumps(out_eval, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  The tallest building in Downtown Jacksonville's skyline is the Bank of America Tower, constructed in 1990 as the Barnett Center. It has a height of 617 ft (188 m) and includes 42 floors. Other notable structures include the 37-story Wells Fargo Center (with its distinctive flared base making it the defining building in the Jacksonville skyline), originally built in 1972-74 by the Independent Life and Accident Insurance Company, and the 28 floor Riverplace Tower which, when completed in 1967, was the tallest precast, post-tensioned concrete structure in the world.\n",
      "\n",
      "Question:  How many stories does the Bank of America tower have?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  In recent years the characteristic that has strongly correlated with health in developed countries is income inequality. Creating an index of \"Health and Social Problems\" from nine factors, authors Richard Wilkinson and Kate Pickett found health and social problems \"more common in countries with bigger income inequalities\", and more common among states in the US with larger income inequalities. Other studies have confirmed this relationship. The UNICEF index of \"child well-being in rich countries\", studying 40 indicators in 22 countries, correlates with greater equality but not per capita income.\n",
      "\n",
      "Question:  How many factors of health and social problems did Wilkinson and PIckett not identify?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  Hamas has continued to be a major player in Palestine. From 2000 to 2007 it killed 542 people in 140 suicide bombing or \"martyrdom operations\". In the January 2006 legislative election—its first foray into the political process—it won the majority of the seats, and in 2007 it drove the PLO out of Gaza. Hamas has been praised by Muslims for driving Israel out of the Gaza Strip, but criticized for failure to achieve its demands in the 2008-9 and 2014 Gaza Wars despite heavy destruction and significant loss of life.\n",
      "\n",
      "Question:   What did Hamas lose in the January 2006 legislative election?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  The Yuan dynasty (Chinese: 元朝; pinyin: Yuán Cháo), officially the Great Yuan (Chinese: 大元; pinyin: Dà Yuán; Mongolian: Yehe Yuan Ulus[a]), was the empire or ruling dynasty of China established by Kublai Khan, leader of the Mongolian Borjigin clan. Although the Mongols had ruled territories including today's North China for decades, it was not until 1271 that Kublai Khan officially proclaimed the dynasty in the traditional Chinese style. His realm was, by this point, isolated from the other khanates and controlled most of present-day China and its surrounding areas, including modern Mongolia and Korea. It was the first foreign dynasty to rule all of China and lasted until 1368, after which its Genghisid rulers returned to their Mongolian homeland and continued to rule the Northern Yuan dynasty. Some of the Mongolian Emperors of the Yuan mastered the Chinese language, while others only used their native language (i.e. Mongolian) and the 'Phags-pa script.\n",
      "\n",
      "Question:  What is the Japanese name for the Yuan dynasty?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted answer:  used\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  When a consolidation referendum was held in 1967, voters approved the plan. On October 1, 1968, the governments merged to create the Consolidated City of Jacksonville. Fire, police, health & welfare, recreation, public works, and housing & urban development were all combined under the new government. In honor of the occasion, then-Mayor Hans Tanzler posed with actress Lee Meredith behind a sign marking the new border of the \"Bold New City of the South\" at Florida 13 and Julington Creek. The Better Jacksonville Plan, promoted as a blueprint for Jacksonville's future and approved by Jacksonville voters in 2000, authorized a half-penny sales tax. This would generate most of the revenue required for the $2.25 billion package of major projects that included road & infrastructure improvements, environmental preservation, targeted economic development and new or improved public facilities.\n",
      "\n",
      "Question:  What was Jacksonville referred to before the consolidation?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    choice = np.random.choice(list(res))\n",
    "    row = test_df[test_df.id == choice].iloc[0]\n",
    "    print(\"Context: \", str(row.context))\n",
    "    print()\n",
    "    print(\"Question: \", str(row.content))\n",
    "    print()\n",
    "    if row.is_impossible:\n",
    "        print(\"Impossible to answer\")\n",
    "    else:\n",
    "        print(\"Answer: \", row.answer)\n",
    "    print()\n",
    "    if res[choice]:\n",
    "        print(\"Predicted answer: \", res[choice])\n",
    "    else:\n",
    "        print(\"Predicted impossbile to answer\")\n",
    "    print(\"\\n//////////////////// \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
