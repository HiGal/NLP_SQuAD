{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squad:\n",
    "    def __init__(self, input_location):\n",
    "        self.location = input_location\n",
    "        file = open(input_location)\n",
    "        json_file = json.load(file)\n",
    "        self.version = json_file['version']\n",
    "        self.data = json_file['data']\n",
    "        \n",
    "        df_builder = []\n",
    "        for sample in self.data:\n",
    "            title = sample['title']\n",
    "            paragraphs = sample['paragraphs']\n",
    "            \n",
    "            for paragraph in paragraphs:\n",
    "                context = paragraph['context']\n",
    "                questions = paragraph['qas']\n",
    "                \n",
    "                for question in questions:\n",
    "                    q_id = question['id']\n",
    "                    q_content = question['question']\n",
    "                    answers = question['answers']\n",
    "                    is_impossible = question['is_impossible']\n",
    "                    qas = {\n",
    "                        'id':q_id,\n",
    "                        'wiki_title':title,\n",
    "                        'context':context,\n",
    "                        'content':q_content,\n",
    "                        'is_impossible':is_impossible\n",
    "                    }\n",
    "                    if is_impossible:\n",
    "                        qas['answer'] = \"\"\n",
    "                        qas['answer_start'] = -1\n",
    "                    else:\n",
    "                        answer = answers[0]\n",
    "                        qas['answer'] = answer['text']\n",
    "                        qas['answer_start'] = answer['answer_start']\n",
    "                    df_builder.append(qas)\n",
    "        self.df = pd.DataFrame(df_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = Squad('./data/train-v2.0.json').df\n",
    "test_df  = Squad('./data/dev-v2.0.json').df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, context_vocab, tgt_vocab, emb_size, num_head, num_encoder_layers=2,\n",
    "                 num_decoder_layers=2, dim_feedforward=756, dropout=0.1, pad_token=1):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.context_emb = nn.Embedding(context_vocab, emb_size)\n",
    "        \n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, emb_size)\n",
    "        \n",
    "        self.transformer = nn.Transformer(emb_size,nhead=num_head,num_encoder_layers=num_encoder_layers,\n",
    "                                         num_decoder_layers=num_decoder_layers,dim_feedforward=dim_feedforward,\n",
    "                                         dropout=dropout)\n",
    "        self.fc_out = nn.Linear(emb_size, tgt_vocab)\n",
    "        self._init_params()\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "    \n",
    "    def forward(self, context, tgt):\n",
    "        context_embedded = self.context_emb(context)\n",
    "        tgt_embedded = self.tgt_emb(tgt)\n",
    "        \n",
    "        src_key_padding_mask = context == self.pad_token\n",
    "        tgt_key_padding_mask = tgt == self.pad_token\n",
    "        \n",
    "        answ = self.transformer(context_embedded, tgt_embedded, src_key_padding_mask=src_key_padding_mask.T,\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask.T)\n",
    "        \n",
    "        output = torch.zeros((answ.shape[0], answ.shape[1], self.tgt_vocab))\n",
    "        for i in answ:\n",
    "            output = model.fc_out(answ)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _init_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['content_q'] = train_df.context +' '+ train_df.content\n",
    "test_df['content_q'] = test_df.context +' '+ test_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from typing import *\n",
    "from torchtext.data import *\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import dill\n",
    "load= False\n",
    "if load:\n",
    "    with open(\"model/CONTEXT_Q.Field\",\"rb\") as f:\n",
    "        CONTEXT=dill.load(f)\n",
    "    with open(\"model/TRG.Field\",\"rb\") as f:\n",
    "        TRG=dill.load(f)\n",
    "else:\n",
    "    CONTEXT_Q = torchtext.data.Field(tokenize = get_tokenizer(\"basic_english\"),\n",
    "                          init_token = '<sos>',\n",
    "                          eos_token = '<eos>',\n",
    "                          lower = True,\n",
    "                          batch_first = False)\n",
    "    \n",
    "    TRG = torchtext.data.Field(tokenize = get_tokenizer(\"basic_english\"), \n",
    "                         init_token = '<sos>',\n",
    "                         eos_token = '<eos>',\n",
    "                         lower = True,\n",
    "                         batch_first = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "class DataFrameExampleSet:\n",
    "    def __init__(self, df, fields):\n",
    "        self._df = df\n",
    "        self._fields = fields\n",
    "        self._fields_dict = {field_name: (field_name, field)\n",
    "                             for field_name, field in fields.items()\n",
    "                             if field is not None}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in tqdm(self._df.itertuples(), total=len(self)):\n",
    "            example = Example.fromdict(item._asdict(), fields=self._fields_dict)\n",
    "            yield example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def shuffle(self, random_state=None):\n",
    "        self._df = self._df.sample(frac=1.0, random_state=random_state)\n",
    "\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, fields, filter_pred=None):\n",
    "        examples = DataFrameExampleSet(df, fields)\n",
    "        super().__init__(examples, fields, filter_pred=filter_pred)\n",
    "\n",
    "\n",
    "class DataFrameBucketIterator(BucketIterator):\n",
    "    def data(self):\n",
    "        if isinstance(self.dataset.examples, DataFrameExampleSet):\n",
    "            if self.shuffle:\n",
    "                self.dataset.examples.shuffle()\n",
    "            dataset = self.dataset\n",
    "        else:\n",
    "            dataset = super().data()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataFrameDataset(train_df, fields={'content_q':CONTEXT_Q,'answer':TRG})\n",
    "test_dataset = DataFrameDataset(test_df, fields={'content_q':CONTEXT_Q,'answer':TRG})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b760bbffc5c94bf69b539e9af0113254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    pass\n",
    "else:\n",
    "    CONTEXT_Q.build_vocab(train_dataset, min_freq=100)\n",
    "    TRG.build_vocab([''], min_freq=100)\n",
    "    TRG.vocab = CONTEXT_Q.vocab\n",
    "    \n",
    "    with open(\"model/CONTEXT_Q.Field\",\"wb+\")as f:\n",
    "        dill.dump(CONTEXT_Q,f)\n",
    "    with open(\"model/TRG.Field\",\"wb+\")as f:\n",
    "        dill.dump(TRG,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (context_emb): Embedding(11814, 512)\n",
       "  (tgt_emb): Embedding(11814, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=11814, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size= 64\n",
    "context_q_vocab = len(CONTEXT_Q.vocab)\n",
    "target_vocab = len(TRG.vocab)\n",
    "emb_size = 512\n",
    "dim_feedforward = 512\n",
    "num_head = 2\n",
    "\n",
    "model = BaselineModel(context_q_vocab, target_vocab, emb_size, num_head, dim_feedforward=dim_feedforward)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has a total of 26,575,398 of trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'Model has a total of {count_trainable_parameters(model):,} of trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = DataFrameBucketIterator.splits((train_dataset, test_dataset), \n",
    "                                    batch_size = batch_size,\n",
    "                                    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=2e-4)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_func):\n",
    "    \"\"\"\n",
    "    Runs training loop for whole dataset in iterator\n",
    "    \n",
    "    model - model to be trained\n",
    "    iterator - data loader from which we take source and target\n",
    "    optimizer - our optimizer\n",
    "    loss_func - function which will compute loss\n",
    "    return average loss\n",
    "    \"\"\"\n",
    "    model.train() # Switch to train\n",
    "    epoch_loss = [] # We will calculate cumulative loss\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        context = batch.content_q\n",
    "        tgt = batch.answer\n",
    "        \n",
    "        output = model(context, tgt)\n",
    "        tgt = tgt.reshape(-1)\n",
    "        output = output.view(-1, output.shape[-1]) \n",
    "\n",
    "        loss = loss_func(output, tgt)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def update_line(hl, x,y):\n",
    "    hl.set_xdata(np.append(hl.get_xdata(), x))\n",
    "    hl.set_ydata(np.append(hl.get_ydata(), y))\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_func):\n",
    "    \"\"\"\n",
    "    Runs an evaluation loop and returns average loss\n",
    "    \n",
    "    model - model to be evaluated\n",
    "    iterator - data loader with validation set\n",
    "    loss_func - function which will compute loss\n",
    "    returns average loss\n",
    "    \"\"\"\n",
    "    model.eval() # Switch to eval\n",
    "    epoch_loss = 0 # We will calculate cumulative loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        to_return = []\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            context = batch.content_q\n",
    "            tgt = batch.answer\n",
    "            \n",
    "            output = model(context, tgt)\n",
    "            \n",
    "            softmaxed = nn.functional.softmax(output, dim=2)\n",
    "            to_return.append(softmaxed.topk(1)[1].squeeze().cpu().detach().numpy())\n",
    "            tgt = tgt.reshape(-1)\n",
    "            output = output.view(-1, output.shape[-1]) \n",
    "            \n",
    "            loss = loss_func(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "    return epoch_loss / len(iterator), to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0876061bc74bbaa3202c6773c51223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d29a8735099439fbafebe2d68dadb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0. Train loss: 0.6893772773145642. Eval loss: 0.06276180865167971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf40beb3824479d9c96f48272c3bec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e173fdc37ec34b898db712dacbb9f06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1. Train loss: 0.025575447855066925. Eval loss: 0.017461498555857487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7360ef447ff480994d4f72229488617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1505dec4e62542108bd116031a2290d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2. Train loss: 0.003849184537743857. Eval loss: 0.011721179549822018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76321f011838474aa7167be2c3b65d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f4c6af56f948989933ac10b3aef6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3. Train loss: 0.00037737141440325725. Eval loss: 0.011264411689847303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a75d82bb8b74835a0a5f924fa87b193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-49de7038c2d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-ebcb47a042db>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, loss_func)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, loss_func)\n",
    "    \n",
    "    eval_loss, preds = evaluate(model, test_iterator, loss_func)\n",
    "    \n",
    "    # save \"best\" model\n",
    "    if best_loss > eval_loss:\n",
    "        best_loss = eval_loss\n",
    "        torch.save(model.state_dict(), 'baseline.model')\n",
    "    print(f\"Epoch {epoch}. Train loss: {np.mean(train_loss)}. Eval loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = preds[0].T\n",
    "first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in first:\n",
    "    TRG.vocab.stoi(''i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[' '.join([TRG.vocab.itos[j] for j in i]) for i in first]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.content_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model(i.content_q, i.answer)\n",
    "            \n",
    "softmaxed = nn.functional.softmax(output, dim=2)\n",
    "softmaxed.topk(1)[1].squeeze().cpu().detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
