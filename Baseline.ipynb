{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline we have build a simple transformer that just takes into the encoder the questions and the context as the input to the decoder and tries to predict the beginning and the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import spacy\n",
    "from torchtext import *\n",
    "from torchtext.data import *\n",
    "import torchtext\n",
    "from typing import *\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch.nn.functional as F\n",
    "import numpy.random as random \n",
    "from evaluate_answers import *\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm') # or use any other tokenizer model for tokenization\n",
    "use_glove = True  ### CHANGE this if you don't want to use GloVe embeddings\n",
    "writer = SummaryWriter()\n",
    "\n",
    "if use_glove:\n",
    "    glove_vocab = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "def tokenizer(text): # create a tokenizer function, you can try using anything else than spacy\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper class to covert json to dataframe for easier batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squad:\n",
    "    \"\"\"Storage for SQuAD dataset\"\"\"\n",
    "    def __init__(self, input_location):\n",
    "        self.location = input_location # Input location to be read\n",
    "        file = open(input_location)\n",
    "        json_file = json.load(file)\n",
    "        # Save version and data\n",
    "        self.version = json_file['version']\n",
    "        self.data = json_file['data']\n",
    "        \n",
    "        df_builder = [] # We will store every row of dataframe here\n",
    "        for sample in self.data:\n",
    "            title = sample['title'] # Get title\n",
    "            paragraphs = sample['paragraphs']\n",
    "            \n",
    "            for paragraph in paragraphs:\n",
    "                context = paragraph['context'] # Get context, e.g. a paragraph\n",
    "                questions = paragraph['qas']\n",
    "                \n",
    "                for question in questions:\n",
    "                    q_id = question['id'] # Question id\n",
    "                    q_content = question['question'] # Question itself\n",
    "                    answers = question['answers'] # Possible answers\n",
    "                    is_impossible = question['is_impossible'] # If it is possible to answer\n",
    "                    \n",
    "                    # Build a row of dataframe\n",
    "                    qas = {\n",
    "                        'id':q_id,\n",
    "                        'wiki_title':title,\n",
    "                        'context':context,\n",
    "                        'content':q_content,\n",
    "                        'is_impossible':is_impossible\n",
    "                    }\n",
    "                    if is_impossible:\n",
    "                        qas['answer'] = \"\"\n",
    "                        qas['answer_start'] = len(context)-1\n",
    "                        qas['answer_end'] =len(context)-1\n",
    "                    else:\n",
    "                        answer = answers[0]\n",
    "                        qas['answer'] = answer['text']\n",
    "                        qas['answer_start'] = answer['answer_start']\n",
    "                        qas['answer_end'] = answer['answer_start']+len(answer['text'])\n",
    "                    df_builder.append(qas) \n",
    "        self.df = pd.DataFrame(df_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sq = Squad('./data/train-v2.0.json') # Load test and train data\n",
    "test_sq = Squad('./data/dev-v2.0.json')\n",
    "train_df  = train_sq.dzf\n",
    "test_df  = test_sq.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from here for easier work with dataframe and torchtext\n",
    "# https://gist.github.com/notnami/3c4d636f2b79e206b26acfe349f2657a\n",
    "class DataFrameExampleSet:\n",
    "    def __init__(self, df, fields):\n",
    "        self._df = df\n",
    "        self._fields = fields\n",
    "        self._fields_dict = {field_name: (field_name, field)\n",
    "                             for field_name, field in fields.items()\n",
    "                             if field is not None}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in tqdm(self._df.itertuples(), total=len(self)):\n",
    "            example = Example.fromdict(item._asdict(), fields=self._fields_dict)\n",
    "            yield example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def shuffle(self, random_state=None):\n",
    "        self._df = self._df.sample(frac=1.0, random_state=random_state)\n",
    "\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, fields, filter_pred=None):\n",
    "        examples = DataFrameExampleSet(df, fields)\n",
    "        super().__init__(examples, fields, filter_pred=filter_pred)\n",
    "\n",
    "\n",
    "class DataFrameBucketIterator(BucketIterator):\n",
    "    def data(self):\n",
    "        if isinstance(self.dataset.examples, DataFrameExampleSet):\n",
    "            if self.shuffle:\n",
    "                self.dataset.examples.shuffle()\n",
    "            dataset = self.dataset\n",
    "        else:\n",
    "            dataset = super().data()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Fields \n",
    "# Here will be context and question\n",
    "CONTEXT = torchtext.data.Field(tokenize = tokenizer,\n",
    "                              lower = False,\n",
    "                              batch_first = False)\n",
    "# here the target \n",
    "QUESTION = torchtext.data.Field(tokenize = tokenizer, \n",
    "                                lower = False,\n",
    "                                batch_first = False)\n",
    "\n",
    "START = torchtext.data.Field(sequential=False, is_target=True, use_vocab=False)\n",
    "END = torchtext.data.Field(sequential=False, is_target=True, use_vocab=False)\n",
    "# Will store id to later check correctness\n",
    "ID = torchtext.data.Field(is_target=True, sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trochtext dataset\n",
    "train_dataset = DataFrameDataset(train_df, fields={'context':CONTEXT,'content':QUESTION, 'id':ID,\n",
    "                                                   'answer_start':START, 'answer_end':END})\n",
    "test_dataset = DataFrameDataset(test_df, fields={'context':CONTEXT,'content':QUESTION, 'id':ID,\n",
    "                                                'answer_start':START, 'answer_end':END})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c1220f5a8b413aaea1310efd134534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from our data\n",
    "# If use glove we will also have vectors for our representations\n",
    "\n",
    "if use_glove:\n",
    "    CONTEXT.build_vocab(train_dataset, vectors='glove.840B.300d')\n",
    "    CONTEXT.vocab.load_vectors('glove.840B.300d')\n",
    "else:\n",
    "    CONTEXT.build_vocab(train_dataset, min_freq=100)\n",
    "QUESTION.build_vocab([''])\n",
    "QUESTION.vocab = CONTEXT.vocab\n",
    "\n",
    "ID.build_vocab(list(train_df.id)+ list(test_df.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Create iterators for test and train\n",
    "train_iterator, test_iterator = DataFrameBucketIterator.splits((train_dataset, test_dataset), \n",
    "                                    batch_size = batch_size,\n",
    "                                    device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Baseline model\"\"\"\n",
    "    def __init__(self, context_vocab, hidden_size, dropout=0.2):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        # If we are using glove that create pretrained embedding layer\n",
    "        if use_glove:\n",
    "            self.context_emb = nn.Embedding.from_pretrained(torch.FloatTensor(CONTEXT.vocab.vectors), freeze=True)\n",
    "            self.question_emb = nn.Embedding.from_pretrained(torch.FloatTensor(CONTEXT.vocab.vectors), freeze=True)\n",
    "            emb_dim = CONTEXT.vocab.vectors.shape[1]\n",
    "        else:\n",
    "            # If not create normal layer\n",
    "            emb_dim = 256\n",
    "            self.context_emb = nn.Embedding(context_vocab, emb_dim)\n",
    "            self.question_emb = nn.Embedding(context_vocab, emb_dim)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Simple transformer with 4 heads and 2 decoder encoder layers\n",
    "        self.rnn = nn.Transformer(d_model=emb_dim, nhead=4, num_encoder_layers=2,\n",
    "                                        num_decoder_layers=2, dim_feedforward=1024,\n",
    "                                        dropout=0.1, activation='relu')\n",
    "                \n",
    "        self.fc_out = nn.Linear(emb_dim, 2)\n",
    "    \n",
    "    def forward(self, context, question, start_positions=None, end_positions=None):\n",
    "        context_embedded = self.dropout_1(self.context_emb(context)) # [context_seq_len, batch_size, emb_size]\n",
    "        question_embedded = self.dropout_2(self.question_emb(question)) # [question_seq_len, batch_size, emb_size]\n",
    "        \n",
    "        output = self.rnn(question_embedded, context_embedded, src_key_padding_mask=question.T==1,\n",
    "                          tgt_key_padding_mask=context.T==1) # [context_seq_len, batch_size, emb_size]\n",
    "        \n",
    "        output_context = output.permute(1,0,2) # [batch_size, context_seq_len, emb_size]\n",
    "        logits = self.fc_out(output_context) # [batch_size, context_seq_len, 2]\n",
    "        start_logits, end_logits = logits.split(1, dim=-1) # split into start and end logits\n",
    "        start_logits = start_logits.squeeze(-1).squeeze(1)\n",
    "        end_logits = end_logits.squeeze(-1).squeeze(1)\n",
    "        \n",
    "        if not start_positions is None and not end_positions is None:\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            # compute loss of start and end (if given)\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss)/2\n",
    "        return total_loss, start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocab = len(CONTEXT.vocab)\n",
    "hidden_size=512\n",
    "\n",
    "model = BaselineModel(context_vocab, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (context_emb): Embedding(100028, 300)\n",
       "  (question_emb): Embedding(100028, 300)\n",
       "  (dropout_1): Dropout(p=0.2, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.2, inplace=False)\n",
       "  (rnn): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "          (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xavier initialization\n",
    "def init_weights(m):\n",
    "    if not isinstance(m, nn.Embedding):\n",
    "        for name, param in m.named_parameters():\n",
    "            if param.data.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has a total of 4,637,898 of trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'Model has a total of {count_trainable_parameters(model):,} of trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer):\n",
    "    \"\"\"\n",
    "    Runs training loop for whole dataset in iterator\n",
    "    \n",
    "    model - model to be trained\n",
    "    iterator - data loader from which we take source and target\n",
    "    optimizer - our optimizer\n",
    "    return average loss\n",
    "    \"\"\"\n",
    "    model.train() # Switch to train\n",
    "    epoch_loss = [] # We will calculate cumulative loss\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        context = batch.context.to(device)\n",
    "        content = batch.content.to(device)\n",
    "        start_positions = batch.answer_start.to(device)\n",
    "        end_positions = batch.answer_end.to(device)\n",
    "        \n",
    "        total_loss, start_logits, end_logits = model(context, content, start_positions, end_positions)\n",
    "    \n",
    "        writer.add_scalar(f'Loss/train Epoch {epoch}', total_loss, i)\n",
    "        \n",
    "        epoch_loss.append(total_loss.item())\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \"\"\"\n",
    "    Runs an evaluation loop and returns average loss\n",
    "    \n",
    "    model - model to be evaluated\n",
    "    iterator - data loader with validation set\n",
    "    returns average loss\n",
    "    \"\"\"\n",
    "    model.eval() # Switch to eval\n",
    "    epoch_loss = 0 # We will calculate cumulative loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        to_return = []\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            context = batch.context.to(device)\n",
    "            content = batch.content.to(device)\n",
    "            start_positions = batch.answer_start.to(device)\n",
    "            end_positions = batch.answer_end.to(device)\n",
    "\n",
    "            total_loss, start_logits, end_logits = model(context, content, start_positions, end_positions)\n",
    "            \n",
    "            start_pred = start_logits.softmax(dim=1).topk(1, dim=1)[1].squeeze().cpu().detach().numpy()\n",
    "            end_pred = end_logits.softmax(dim=1).topk(1, dim=1)[1].squeeze().cpu().detach().numpy()\n",
    "            \n",
    "            to_return.append((start_pred, end_pred, batch.id))\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "    return epoch_loss / len(iterator), to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(predictions, labels, df):\n",
    "    # Transform our predictions from start-end to the text\n",
    "    my_preds = {}\n",
    "\n",
    "    for pred, tgt in zip(predictions, labels):\n",
    "        start, end = pred\n",
    "        tg_id = ID.vocab.itos[tgt]\n",
    "        res = df[df.id == tg_id].context.values[0][start:end]\n",
    "        my_preds[ID.vocab.itos[tgt]] = res\n",
    "    return my_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(preds, squad):\n",
    "    # flatten output\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in preds:\n",
    "        for seq1,seq2, tgt in zip(i[0], i[1], i[2]):\n",
    "            predictions.append((seq1,seq2))\n",
    "            labels.append(tgt)\n",
    "    my_preds = get_preds(predictions, labels, test_df) # Get predictions\n",
    "    \n",
    "    # For the more representetive results we have taken script that squad owner's have written to check predictions\n",
    "    dataset = squad.data\n",
    "    preds = my_preds\n",
    "    na_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "    qid_to_has_ans = make_qid_to_has_ans(dataset) \n",
    "    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "    exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "    exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
    "                                          1.0)\n",
    "    f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
    "                                       1.0)\n",
    "    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "    if has_ans_qids:\n",
    "        has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "        merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
    "    if no_ans_qids:\n",
    "        no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "        merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
    "    print(json.dumps(out_eval, indent=2))\n",
    "    \n",
    "    return my_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f388d02e94448bbd6fcd8929e0139c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d7577ebb5e4419816db648f5eb68b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"exact\": 43.37572643813695,\n",
      "  \"f1\": 43.8093533605187,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 0.0,\n",
      "  \"HasAns_f1\": 0.8684973767608978,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 86.62741799831791,\n",
      "  \"NoAns_f1\": 86.62741799831791,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n",
      "Epoch 0. Train loss: 5.823935319584963. Eval loss: 5.379476803605274\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer)\n",
    "    \n",
    "    eval_loss, model_preds = evaluate(model, test_iterator)\n",
    "    \n",
    "    compute_results(model_preds, test_sq)\n",
    "    # save \"best\" model\n",
    "    if best_loss > eval_loss:\n",
    "        best_loss = eval_loss\n",
    "        torch.save(model.state_dict(), 'baseline.model')\n",
    "    print(f\"Epoch {epoch}. Train loss: {np.mean(train_loss)}. Eval loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 43.37572643813695,\n",
      "  \"f1\": 43.8093533605187,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 0.0,\n",
      "  \"HasAns_f1\": 0.8684973767608978,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 86.62741799831791,\n",
      "  \"NoAns_f1\": 86.62741799831791,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = compute_results(model_preds, test_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some samples of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Harvard has been highly ranked by many university rankings. In particular, it has consistently topped the Academic Ranking of World Universities (ARWU) since 2003, and the THE World Reputation Rankings since 2011, when the first time such league tables were published. When the QS and Times were published in partnership as the THE-QS World University Rankings during 2004-2009, Harvard had also been regarded the first in every year. The University's undergraduate program has been continuously among the top two in the U.S. News & World Report. In 2014, Harvard topped the University Ranking by Academic Performance (URAP). It was ranked 8th on the 2013-2014 PayScale College Salary Report and 14th on the 2013 PayScale College Education Value Rankings. From a poll done by The Princeton Review, Harvard is the second most commonly named \"dream college\", both for students and parents in 2013, and was the first nominated by parents in 2009. In 2011, the Mines ParisTech : Professional Ranking World Universities ranked Harvard 1st university in the world in terms of number of alumni holding CEO position in Fortune Global 500 companies.\n",
      "\n",
      "Question:  Beginning in what year was Harvard on top of the World Reputation Rankings?\n",
      "\n",
      "Answer:  2011\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  The French acquired a copy of the British war plans, including the activities of Shirley and Johnson. Shirley's efforts to fortify Oswego were bogged down in logistical difficulties, exacerbated by Shirley's inexperience in managing large expeditions. In conjunction, Shirley was made aware that the French were massing for an attack on Fort Oswego in his absence when he planned to attack Fort Niagara. As a response, Shirley left garrisons at Oswego, Fort Bull, and Fort Williams (the latter two located on the Oneida Carry between the Mohawk River and Wood Creek at present-day Rome, New York). Supplies for use in the projected attack on Niagara were cached at Fort Bull.\n",
      "\n",
      "Question:  Where was Shirey going to be when Fort Oswego was to be attacked?\n",
      "\n",
      "Answer:  planned to attack Fort Niagara\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  In India, private schools are called independent schools, but since some private schools receive financial aid from the government, it can be an aided or an unaided school. So, in a strict sense, a private school is an unaided independent school. For the purpose of this definition, only receipt of financial aid is considered, not land purchased from the government at a subsidized rate. It is within the power of both the union government and the state governments to govern schools since Education appears in the Concurrent list of legislative subjects in the constitution. The practice has been for the union government to provide the broad policy directions while the states create their own rules and regulations for the administration of the sector. Among other things, this has also resulted in 30 different Examination Boards or academic authorities that conduct examinations for school leaving certificates. Prominent Examination Boards that are present in multiple states are the CBSE and the CISCE, NENBSE\n",
      "\n",
      "Question:  For what certificate does the legislature conduct examinations?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  The new British command was not in place until July. When he arrived in Albany, Abercrombie refused to take any significant actions until Loudoun approved them. Montcalm took bold action against his inertia. Building on Vaudreuil's work harassing the Oswego garrison, Montcalm executed a strategic feint by moving his headquarters to Ticonderoga, as if to presage another attack along Lake George. With Abercrombie pinned down at Albany, Montcalm slipped away and led the successful attack on Oswego in August. In the aftermath, Montcalm and the Indians under his command disagreed about the disposition of prisoners' personal effects. The Europeans did not consider them prizes and prevented the Indians from stripping the prisoners of their valuables, which angered the Indians.\n",
      "\n",
      "Question:  What disagreement did Montcalm and Indians have?\n",
      "\n",
      "Answer:  disposition of prisoners' personal effects\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  Some Huguenots settled in Bedfordshire, one of the main centres of the British lace industry at the time. Although 19th century sources have asserted that some of these refugees were lacemakers and contributed to the East Midlands lace industry, this is contentious. The only reference to immigrant lacemakers in this period is of twenty-five widows who settled in Dover, and there is no contemporary documentation to support there being Huguenot lacemakers in Bedfordshire. The implication that the style of lace known as 'Bucks Point' demonstrates a Huguenot influence, being a \"combination of Mechlin patterns on Lille ground\", is fallacious: what is now known as Mechlin lace did not develop until first half of the eighteenth century and lace with Mechlin patterns and Lille ground did not appear until the end of the 18th century, when it was widely copied throughout Europe.\n",
      "\n",
      "Question:  In what era was \"Bucks Point\" lace making developed?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    choice = np.random.choice(list(res))\n",
    "    row = test_df[test_df.id == choice].iloc[0]\n",
    "    print(\"Context: \", str(row.context))\n",
    "    print()\n",
    "    print(\"Question: \", str(row.content))\n",
    "    print()\n",
    "    if row.is_impossible:\n",
    "        print(\"Impossible to answer\")\n",
    "    else:\n",
    "        print(\"Answer: \", row.answer)\n",
    "    print()\n",
    "    if res[choice]:\n",
    "        print(\"Predicted answer: \", res[choice])\n",
    "    else:\n",
    "        print(\"Predicted impossbile to answer\")\n",
    "    print(\"\\n//////////////////// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
