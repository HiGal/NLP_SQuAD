{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline we have build a simple transformer that just has as an input context+question and tries to predict an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper class to covert json to dataframe for easier batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squad:\n",
    "    def __init__(self, input_location):\n",
    "        self.location = input_location\n",
    "        file = open(input_location)\n",
    "        json_file = json.load(file)\n",
    "        # Save version and data\n",
    "        self.version = json_file['version']\n",
    "        self.data = json_file['data']\n",
    "        \n",
    "        df_builder = [] # We will store every row of dataframe here\n",
    "        for sample in self.data:\n",
    "            title = sample['title'] # Get title\n",
    "            paragraphs = sample['paragraphs']\n",
    "            \n",
    "            for paragraph in paragraphs:\n",
    "                context = paragraph['context'] # Get context, e.g. a paragraph\n",
    "                questions = paragraph['qas']\n",
    "                \n",
    "                for question in questions:\n",
    "                    q_id = question['id'] # Question id\n",
    "                    q_content = question['question'] # Question itself\n",
    "                    answers = question['answers'] # Possible answers\n",
    "                    is_impossible = question['is_impossible'] # If it is possible to answer\n",
    "                    \n",
    "                    # Build a row of dataframe\n",
    "                    qas = {\n",
    "                        'id':q_id,\n",
    "                        'wiki_title':title,\n",
    "                        'context':context,\n",
    "                        'content':q_content,\n",
    "                        'is_impossible':is_impossible\n",
    "                    }\n",
    "                    if is_impossible:\n",
    "                        qas['answer'] = \"\"\n",
    "                        qas['answer_start'] = -1\n",
    "                    else:\n",
    "                        answer = answers[0]\n",
    "                        qas['answer'] = answer['text']\n",
    "                        qas['answer_start'] = answer['answer_start']\n",
    "                    df_builder.append(qas) \n",
    "        self.df = pd.DataFrame(df_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sq = Squad('./data/train-v2.0.json')\n",
    "test_sq = Squad('./data/dev-v2.0.json')\n",
    "train_df  = train_sq.df\n",
    "test_df  = test_sq.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"A very simplistic model to predict answers\"\"\"\n",
    "    def __init__(self, context_vocab, tgt_vocab, emb_size, num_head, num_encoder_layers=2,\n",
    "                 num_decoder_layers=2, dim_feedforward=756, dropout=0.1, pad_token=1):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.context_emb = nn.Embedding(context_vocab, emb_size) # We embed context + question here\n",
    "        \n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, emb_size) # We embed target here\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(emb_size, num_head, dim_feedforward, dropout, 'relu')\n",
    "        encoder_norm = nn.LayerNorm(emb_size)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(emb_size, num_head, dim_feedforward, dropout, 'relu')\n",
    "        decoder_norm = nn.LayerNorm(emb_size)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        \n",
    "        # Linear layer to predict the output\n",
    "        self.fc_out = nn.Linear(emb_size, tgt_vocab)\n",
    "        self._init_params()\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "    \n",
    "    def forward(self, context, tgt=None, teacher_forcing=0.75):\n",
    "        \n",
    "        # Embed context and target\n",
    "        context_embedded = self.context_emb(context)\n",
    "        \n",
    "        src_key_padding_mask = context == self.pad_token\n",
    "        \n",
    "        \n",
    "        memory = self.encoder(context_embedded, mask=None, src_key_padding_mask=src_key_padding_mask.T)\n",
    "        \n",
    "        batch_size = context.shape[1]\n",
    "        inp = torch.LongTensor([2]*batch_size).to(device).unsqueeze(0)\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(1, TRG_LEN):\n",
    "            inp_embedded = self.tgt_emb(inp)\n",
    "            output = self.decoder(inp_embedded, memory)\n",
    "            output_fc = model.fc_out(output)\n",
    "            \n",
    "            if outputs is None:\n",
    "                outputs = output_fc\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, output_fc), dim=0)\n",
    "                \n",
    "            inp = output.argmax(2)\n",
    "            if np.random.random() < teacher_forcing and not len(tgt) <= i: # Try teaching forcing\n",
    "                inp = tgt[i, :].unsqueeze(0)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _init_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "from torchtext.data import *\n",
    "\n",
    "# Taken from here for easier work with dataframe and torchtext\n",
    "# https://gist.github.com/notnami/3c4d636f2b79e206b26acfe349f2657a\n",
    "class DataFrameExampleSet:\n",
    "    def __init__(self, df, fields):\n",
    "        self._df = df\n",
    "        self._fields = fields\n",
    "        self._fields_dict = {field_name: (field_name, field)\n",
    "                             for field_name, field in fields.items()\n",
    "                             if field is not None}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in tqdm(self._df.itertuples(), total=len(self)):\n",
    "            example = Example.fromdict(item._asdict(), fields=self._fields_dict)\n",
    "            yield example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def shuffle(self, random_state=None):\n",
    "        self._df = self._df.sample(frac=1.0, random_state=random_state)\n",
    "\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, fields, filter_pred=None):\n",
    "        examples = DataFrameExampleSet(df, fields)\n",
    "        super().__init__(examples, fields, filter_pred=filter_pred)\n",
    "\n",
    "\n",
    "class DataFrameBucketIterator(BucketIterator):\n",
    "    def data(self):\n",
    "        if isinstance(self.dataset.examples, DataFrameExampleSet):\n",
    "            if self.shuffle:\n",
    "                self.dataset.examples.shuffle()\n",
    "            dataset = self.dataset\n",
    "        else:\n",
    "            dataset = super().data()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate content and context\n",
    "train_df['content_q'] = train_df.context +' '+ train_df.content\n",
    "test_df['content_q'] = test_df.context +' '+ test_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from typing import *\n",
    "from torchtext.data import *\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import dill\n",
    "load = False\n",
    "TRG_LEN=15\n",
    "\n",
    "if load:\n",
    "    with open(\"model/CONTEXT_Q.Field\",\"rb\") as f:\n",
    "        CONTEXT_Q=dill.load(f)\n",
    "    with open(\"model/TRG.Field\",\"rb\") as f:\n",
    "        TRG=dill.load(f)\n",
    "else:\n",
    "    # Init Fields \n",
    "    \n",
    "    # Here will be context and question\n",
    "    CONTEXT_Q = torchtext.data.Field(tokenize = get_tokenizer(\"basic_english\"),\n",
    "                          init_token = '<sos>',\n",
    "                          eos_token = '<eos>',\n",
    "                          lower = True,\n",
    "                          batch_first = False)\n",
    "    # here the target \n",
    "    TRG = torchtext.data.Field(tokenize = get_tokenizer(\"basic_english\"), \n",
    "                         init_token = '<sos>',\n",
    "                         eos_token = '<eos>',\n",
    "                         lower = True,\n",
    "                         batch_first = False,\n",
    "                         fix_length=TRG_LEN)\n",
    "    \n",
    "# Will store id to later check correctness\n",
    "ID = torchtext.data.Field(is_target=True, sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataFrameDataset(train_df, fields={'content_q':CONTEXT_Q,'answer':TRG, 'id':ID})\n",
    "test_dataset = DataFrameDataset(test_df, fields={'content_q':CONTEXT_Q,'answer':TRG, 'id':ID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b47df08d94344358b7e4ef237b8000a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    pass\n",
    "else:\n",
    "    # Build vocabulary from our data, target will have the same vocab as context + questions\n",
    "    CONTEXT_Q.build_vocab(train_dataset, min_freq=150)\n",
    "    TRG.build_vocab([''], min_freq=150)\n",
    "    TRG.vocab = CONTEXT_Q.vocab\n",
    "    \n",
    "    \n",
    "    with open(\"model/CONTEXT_Q.Field\",\"wb+\")as f:\n",
    "        dill.dump(CONTEXT_Q,f)\n",
    "    with open(\"model/TRG.Field\",\"wb+\")as f:\n",
    "        dill.dump(TRG,f)\n",
    "ID.build_vocab(list(train_df.id)+ list(test_df.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Create iterators\n",
    "train_iterator, test_iterator = DataFrameBucketIterator.splits((train_dataset, test_dataset), \n",
    "                                    batch_size = batch_size,\n",
    "                                    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (context_emb): Embedding(9138, 512)\n",
       "  (tgt_emb): Embedding(9138, 512)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=9138, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_q_vocab = len(CONTEXT_Q.vocab)\n",
    "target_vocab = len(TRG.vocab)\n",
    "emb_size = 512\n",
    "dim_feedforward = 512\n",
    "num_head = 1\n",
    "\n",
    "# Init model\n",
    "model = BaselineModel(context_q_vocab, target_vocab, emb_size, num_head, dim_feedforward=dim_feedforward)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has a total of 22,462,386 of trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'Model has a total of {count_trainable_parameters(model):,} of trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=2e-4)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_func):\n",
    "    \"\"\"\n",
    "    Runs training loop for whole dataset in iterator\n",
    "    \n",
    "    model - model to be trained\n",
    "    iterator - data loader from which we take source and target\n",
    "    optimizer - our optimizer\n",
    "    loss_func - function which will compute loss\n",
    "    return average loss\n",
    "    \"\"\"\n",
    "    model.train() # Switch to train\n",
    "    epoch_loss = [] # We will calculate cumulative loss\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        context = batch.content_q\n",
    "        tgt = batch.answer\n",
    "        \n",
    "        output = model(context, tgt)\n",
    "        tgt = tgt[1:].reshape(-1)\n",
    "        output = output.view(-1, output.shape[-1]) \n",
    "        \n",
    "        loss = loss_func(output, tgt)\n",
    "        writer.add_scalar(f'Loss/train', loss, i)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_func):\n",
    "    \"\"\"\n",
    "    Runs an evaluation loop and returns average loss\n",
    "    \n",
    "    model - model to be evaluated\n",
    "    iterator - data loader with validation set\n",
    "    loss_func - function which will compute loss\n",
    "    returns average loss\n",
    "    \"\"\"\n",
    "    model.eval() # Switch to eval\n",
    "    epoch_loss = 0 # We will calculate cumulative loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        to_return = []\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            context = batch.content_q\n",
    "            tgt = batch.answer\n",
    "            \n",
    "            output = model(context, teacher_forcing=0)\n",
    "            \n",
    "            softmaxed = nn.functional.softmax(output, dim=2)\n",
    "            # store ids of batch and result\n",
    "            to_return.append((softmaxed.topk(1)[1].squeeze().cpu().detach().numpy(), batch.id)) \n",
    "            \n",
    "            tgt = tgt[1:].reshape(-1)\n",
    "            output = output.view(-1, output.shape[-1]) \n",
    "            \n",
    "            loss = loss_func(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "    return epoch_loss / len(iterator), to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3862d70b3542549e49a6b65d4cfb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, loss_func)\n",
    "    \n",
    "    eval_loss, preds = evaluate(model, test_iterator, loss_func)\n",
    "    \n",
    "    # save \"best\" model\n",
    "    if best_loss > eval_loss:\n",
    "        best_loss = eval_loss\n",
    "        torch.save(model.state_dict(), 'baseline.model')\n",
    "    print(f\"Epoch {epoch}. Train loss: {np.mean(train_loss)}. Eval loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = evaluate(model, test_iterator, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten output\n",
    "predictions = []\n",
    "labels = []\n",
    "for i in preds:\n",
    "    for seq, tgt in zip(i[0].T, i[1]):\n",
    "        if isinstance(seq, np.ndarray):\n",
    "            predictions.append(seq)\n",
    "        labels.append(tgt.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(predictions):\n",
    "    # Converting to string our predictions\n",
    "    to_return = []\n",
    "    for pred in predictions:\n",
    "        result = []\n",
    "        for word in pred:\n",
    "            if word == 3: # If eos token - break\n",
    "                break\n",
    "            if word != 2: # If sos token - skip\n",
    "                result.append(TRG.vocab.itos[word])\n",
    "        to_return.append(' '.join(result))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correctness(predictions, labels, df):\n",
    "    # Transform our predictions and check their correctness\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    answers = {}\n",
    "    for i, row in df.iterrows():\n",
    "        answers[row.id] = ' '.join(tokenizer(row.answer))\n",
    "    \n",
    "    correct = {}\n",
    "    my_preds = {}\n",
    "    for pred, tgt in zip(predictions, labels):\n",
    "        correct[ID.vocab.itos[tgt]] = answers[ID.vocab.itos[tgt]] == pred\n",
    "        my_preds[ID.vocab.itos[tgt]] = pred\n",
    "        \n",
    "    correct_preds = answers\n",
    "    return correct, correct_preds, my_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = to_string(predictions)\n",
    "correct, correct_preds, my_preds = check_correctness(predictions, labels, test_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "for sample in test_sq.data:\n",
    "    for paragraph in sample['paragraphs']:\n",
    "        for question in paragraph['qas']:\n",
    "            q_id = question['id']\n",
    "            is_impossible = question['is_impossible']\n",
    "            if not is_impossible:\n",
    "                question['answers'][0]['text'] = correct_preds[q_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the more representetive results we have taken script that squad owner's have written to check predictions\n",
    "\n",
    "dataset = test_sq.data\n",
    "preds = my_preds\n",
    "na_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "qid_to_has_ans = make_qid_to_has_ans(dataset) \n",
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
    "                                      1.0)\n",
    "f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
    "                                   1.0)\n",
    "out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "if has_ans_qids:\n",
    "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
    "if no_ans_qids:\n",
    "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
    "print(json.dumps(out_eval, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some samples of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    choice = np.random.choice(list(my_preds))\n",
    "    row = test_df[test_df.id == choice].iloc[0]\n",
    "    print(\"Context: \", str(row.context))\n",
    "    print()\n",
    "    print(\"Question: \", str(row.content))\n",
    "    print()\n",
    "    if row.is_impossible:\n",
    "        print(\"Impossible to answer\")\n",
    "    else:\n",
    "        print(\"Answer: \", row.answer)\n",
    "    print()\n",
    "    if my_preds[choice]:\n",
    "        print(\"Predicted answer: \", my_preds[choice])\n",
    "    else:\n",
    "        print(\"Predicted impossbile to answer\")\n",
    "    print(\"\\n//////////////////// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
