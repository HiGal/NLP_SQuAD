{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline we have build a simple transformer that just has as an input context+question and tries to predict an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper class to covert json to dataframe for easier batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squad:\n",
    "    def __init__(self, input_location):\n",
    "        self.location = input_location\n",
    "        file = open(input_location)\n",
    "        json_file = json.load(file)\n",
    "        # Save version and data\n",
    "        self.version = json_file['version']\n",
    "        self.data = json_file['data']\n",
    "        \n",
    "        df_builder = [] # We will store every row of dataframe here\n",
    "        for sample in self.data:\n",
    "            title = sample['title'] # Get title\n",
    "            paragraphs = sample['paragraphs']\n",
    "            \n",
    "            for paragraph in paragraphs:\n",
    "                context = paragraph['context'] # Get context, e.g. a paragraph\n",
    "                questions = paragraph['qas']\n",
    "                \n",
    "                for question in questions:\n",
    "                    q_id = question['id'] # Question id\n",
    "                    q_content = question['question'] # Question itself\n",
    "                    answers = question['answers'] # Possible answers\n",
    "                    is_impossible = question['is_impossible'] # If it is possible to answer\n",
    "                    \n",
    "                    # Build a row of dataframe\n",
    "                    qas = {\n",
    "                        'id':q_id,\n",
    "                        'wiki_title':title,\n",
    "                        'context':context,\n",
    "                        'content':q_content,\n",
    "                        'is_impossible':is_impossible\n",
    "                    }\n",
    "                    if is_impossible:\n",
    "                        qas['answer'] = \"\"\n",
    "                        qas['answer_start'] = -1\n",
    "                    else:\n",
    "                        answer = answers[0]\n",
    "                        qas['answer'] = answer['text']\n",
    "                        qas['answer_start'] = answer['answer_start']\n",
    "                    df_builder.append(qas) \n",
    "        self.df = pd.DataFrame(df_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sq = Squad('./data/train-v2.0.json')\n",
    "test_sq = Squad('./data/dev-v2.0.json')\n",
    "train_df  = train_sq.df\n",
    "test_df  = test_sq.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"A very simplistic model to predict answers\"\"\"\n",
    "    def __init__(self, context_vocab, tgt_vocab, emb_size, num_head, num_encoder_layers=2,\n",
    "                 num_decoder_layers=2, dim_feedforward=756, dropout=0.1, pad_token=1):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.context_emb = nn.Embedding(context_vocab, emb_size) # We embed context + question here\n",
    "        \n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab, emb_size) # We embed target here\n",
    "        \n",
    "        # Transformer in which we input the context and target\n",
    "#         self.transformer = nn.Transformer(emb_size,nhead=num_head,num_encoder_layers=num_encoder_layers,\n",
    "#                                          num_decoder_layers=num_decoder_layers,dim_feedforward=dim_feedforward,\n",
    "#                                          dropout=dropout)\n",
    "        \n",
    "#         encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "#         encoder_norm = LayerNorm(d_model)\n",
    "#         self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "\n",
    "#         decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "#         decoder_norm = LayerNorm(d_model)\n",
    "#         self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        \n",
    "        # Linear layer to predict the output\n",
    "        self.fc_out = nn.Linear(emb_size, tgt_vocab)\n",
    "        self._init_params()\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "    \n",
    "    def forward(self, context, tgt):\n",
    "        \n",
    "        # Embed context and target\n",
    "        context_embedded = self.context_emb(context)\n",
    "        tgt_embedded = self.tgt_emb(tgt)\n",
    "        \n",
    "        src_key_padding_mask = context == self.pad_token\n",
    "        tgt_key_padding_mask = tgt == self.pad_token\n",
    "        \n",
    "        # Transform context and target\n",
    "        answ = self.transformer(context_embedded, tgt_embedded, src_key_padding_mask=src_key_padding_mask.T,\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask.T)\n",
    "        \n",
    "        output = torch.zeros((answ.shape[0], answ.shape[1], self.tgt_vocab))\n",
    "        for i in answ:\n",
    "            output = model.fc_out(answ)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _init_params(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "from torchtext.data import *\n",
    "\n",
    "# Taken from here for easier work with dataframe and torchtext\n",
    "# https://gist.github.com/notnami/3c4d636f2b79e206b26acfe349f2657a\n",
    "class DataFrameExampleSet:\n",
    "    def __init__(self, df, fields):\n",
    "        self._df = df\n",
    "        self._fields = fields\n",
    "        self._fields_dict = {field_name: (field_name, field)\n",
    "                             for field_name, field in fields.items()\n",
    "                             if field is not None}\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in tqdm(self._df.itertuples(), total=len(self)):\n",
    "            example = Example.fromdict(item._asdict(), fields=self._fields_dict)\n",
    "            yield example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "    def shuffle(self, random_state=None):\n",
    "        self._df = self._df.sample(frac=1.0, random_state=random_state)\n",
    "\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, fields, filter_pred=None):\n",
    "        examples = DataFrameExampleSet(df, fields)\n",
    "        super().__init__(examples, fields, filter_pred=filter_pred)\n",
    "\n",
    "\n",
    "class DataFrameBucketIterator(BucketIterator):\n",
    "    def data(self):\n",
    "        if isinstance(self.dataset.examples, DataFrameExampleSet):\n",
    "            if self.shuffle:\n",
    "                self.dataset.examples.shuffle()\n",
    "            dataset = self.dataset\n",
    "        else:\n",
    "            dataset = super().data()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate content and context\n",
    "train_df['content_q'] = train_df.context +' '+ train_df.content\n",
    "test_df['content_q'] = test_df.context +' '+ test_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from typing import *\n",
    "from torchtext.data import *\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import dill\n",
    "load= False\n",
    "if load:\n",
    "    with open(\"model/CONTEXT_Q.Field\",\"rb\") as f:\n",
    "        CONTEXT=dill.load(f)\n",
    "    with open(\"model/TRG.Field\",\"rb\") as f:\n",
    "        TRG=dill.load(f)\n",
    "else:\n",
    "    # Init Fields \n",
    "    \n",
    "    # Here will be context and question\n",
    "    CONTEXT_Q = torchtext.data.Field(tokenize = get_tokenizer(\"toktok\"),\n",
    "                          init_token = '<sos>',\n",
    "                          eos_token = '<eos>',\n",
    "                          lower = False,\n",
    "                          batch_first = False)\n",
    "    # here the target \n",
    "    TRG = torchtext.data.Field(tokenize = get_tokenizer(\"toktok\"), \n",
    "                         init_token = '<sos>',\n",
    "                         eos_token = '<eos>',\n",
    "                         lower = False,\n",
    "                         batch_first = False)\n",
    "    \n",
    "# Will store id to later check correctness\n",
    "ID = torchtext.data.Field(is_target=True, sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataFrameDataset(train_df, fields={'content_q':CONTEXT_Q,'answer':TRG, 'id':ID})\n",
    "test_dataset = DataFrameDataset(test_df, fields={'content_q':CONTEXT_Q,'answer':TRG, 'id':ID})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15199100f14a4139a6c4dfab1feaf54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    pass\n",
    "else:\n",
    "    # Build vocabulary from our data, target will have the same vocab as context + questions\n",
    "    CONTEXT_Q.build_vocab(train_dataset, min_freq=50)\n",
    "    TRG.build_vocab([''], min_freq=50)\n",
    "    TRG.vocab = CONTEXT_Q.vocab\n",
    "    \n",
    "    \n",
    "    with open(\"model/CONTEXT_Q.Field\",\"wb+\")as f:\n",
    "        dill.dump(CONTEXT_Q,f)\n",
    "    with open(\"model/TRG.Field\",\"wb+\")as f:\n",
    "        dill.dump(TRG,f)\n",
    "ID.build_vocab(list(train_df.id)+ list(test_df.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "# Create iterators\n",
    "train_iterator, test_iterator = DataFrameBucketIterator.splits((train_dataset, test_dataset), \n",
    "                                    batch_size = batch_size,\n",
    "                                    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (context_emb): Embedding(21931, 512)\n",
       "  (tgt_emb): Embedding(21931, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=21931, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_q_vocab = len(CONTEXT_Q.vocab)\n",
    "target_vocab = len(TRG.vocab)\n",
    "emb_size = 512\n",
    "dim_feedforward = 512\n",
    "num_head = 2\n",
    "\n",
    "# Init model\n",
    "model = BaselineModel(context_q_vocab, target_vocab, emb_size, num_head, dim_feedforward=dim_feedforward)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has a total of 42,125,227 of trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'Model has a total of {count_trainable_parameters(model):,} of trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=2e-4)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, loss_func):\n",
    "    \"\"\"\n",
    "    Runs training loop for whole dataset in iterator\n",
    "    \n",
    "    model - model to be trained\n",
    "    iterator - data loader from which we take source and target\n",
    "    optimizer - our optimizer\n",
    "    loss_func - function which will compute loss\n",
    "    return average loss\n",
    "    \"\"\"\n",
    "    model.train() # Switch to train\n",
    "    epoch_loss = [] # We will calculate cumulative loss\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        context = batch.content_q\n",
    "        tgt = batch.answer\n",
    "        \n",
    "        output = model(context, tgt)\n",
    "        tgt = tgt.reshape(-1)\n",
    "        output = output.view(-1, output.shape[-1]) \n",
    "\n",
    "        loss = loss_func(output, tgt)\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_func):\n",
    "    \"\"\"\n",
    "    Runs an evaluation loop and returns average loss\n",
    "    \n",
    "    model - model to be evaluated\n",
    "    iterator - data loader with validation set\n",
    "    loss_func - function which will compute loss\n",
    "    returns average loss\n",
    "    \"\"\"\n",
    "    model.eval() # Switch to eval\n",
    "    epoch_loss = 0 # We will calculate cumulative loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        to_return = []\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            context = batch.content_q\n",
    "            tgt = batch.answer\n",
    "            \n",
    "            output = model(context, tgt)\n",
    "            \n",
    "            softmaxed = nn.functional.softmax(output, dim=2)\n",
    "            # store ids of batch and result\n",
    "            to_return.append((softmaxed.topk(1)[1].squeeze().cpu().detach().numpy(), batch.id)) \n",
    "            \n",
    "            tgt = tgt.reshape(-1)\n",
    "            output = output.view(-1, output.shape[-1]) \n",
    "            \n",
    "            loss = loss_func(output, tgt)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "    return epoch_loss / len(iterator), to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d31e850ee54894a1202519eb232cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680f04b5c8254f308d07584c4b1ed8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0. Train loss: 0.9920720200533953. Eval loss: 0.1669959529473256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0acec5fa8943cfa70fe177a43be431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9644542c5942349e7481e4990d02b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1. Train loss: 0.07054533362387468. Eval loss: 0.052245992279651585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4112f8cf9356430397f7cd79ad0bbe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ced3ec80c14197ba26cce003acef78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2. Train loss: 0.010890705061757664. Eval loss: 0.0357376340998403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446c56367ba044b9b92487fb7a1996ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130319.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15afc875e7364632af030ca087ecd258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3. Train loss: 0.0006800501947731825. Eval loss: 0.03633174848578996\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_iterator, optimizer, loss_func)\n",
    "    \n",
    "    eval_loss, preds = evaluate(model, test_iterator, loss_func)\n",
    "    \n",
    "    # save \"best\" model\n",
    "    if best_loss > eval_loss:\n",
    "        best_loss = eval_loss\n",
    "        torch.save(model.state_dict(), 'baseline.model')\n",
    "    print(f\"Epoch {epoch}. Train loss: {np.mean(train_loss)}. Eval loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6ccc9123a4408db20517535ce26128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = evaluate(model, test_iterator, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten output\n",
    "predictions = []\n",
    "labels = []\n",
    "for i in preds:\n",
    "    for seq, tgt in zip(i[0].T, i[1]):\n",
    "        predictions.append(seq)\n",
    "        labels.append(tgt.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(predictions):\n",
    "    # Converting to string our predictions\n",
    "    to_return = []\n",
    "    for pred in predictions:\n",
    "        result = []\n",
    "        for word in pred:\n",
    "            if word == 3: # If eos token - break\n",
    "                break\n",
    "            if word != 2: # If sos token - skip\n",
    "                result.append(TRG.vocab.itos[word])\n",
    "        to_return.append(' '.join(result))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correctness(predictions, labels, df):\n",
    "    # Transform our predictions and check their correctness\n",
    "    tokenizer = get_tokenizer(\"toktok\")\n",
    "    answers = {}\n",
    "    for i, row in df.iterrows():\n",
    "        answers[row.id] = ' '.join(tokenizer(row.answer))\n",
    "    \n",
    "    correct = {}\n",
    "    my_preds = {}\n",
    "    for pred, tgt in zip(predictions, labels):\n",
    "        correct[ID.vocab.itos[tgt]] = answers[ID.vocab.itos[tgt]] == pred\n",
    "        my_preds[ID.vocab.itos[tgt]] = pred\n",
    "        \n",
    "    correct_preds = answers\n",
    "    return correct, correct_preds, my_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = to_string(predictions)\n",
    "correct, correct_preds, my_preds = check_correctness(predictions, labels, test_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "for sample in test_sq.data:\n",
    "    for paragraph in sample['paragraphs']:\n",
    "        for question in paragraph['qas']:\n",
    "            q_id = question['id']\n",
    "            is_impossible = question['is_impossible']\n",
    "            if not is_impossible:\n",
    "                question['answers'][0]['text'] = correct_preds[q_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 77.98366040596311,\n",
      "  \"f1\": 86.79409211178795,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 55.904183535762485,\n",
      "  \"HasAns_f1\": 73.55031303023961,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 100.0,\n",
      "  \"NoAns_f1\": 100.0,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# For the more representetive results we have taken script that squad owner's have written to check predictions\n",
    "\n",
    "dataset = test_sq.data\n",
    "preds = my_preds\n",
    "na_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "qid_to_has_ans = make_qid_to_has_ans(dataset) \n",
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
    "exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
    "                                      1.0)\n",
    "f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
    "                                   1.0)\n",
    "out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
    "if has_ans_qids:\n",
    "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
    "if no_ans_qids:\n",
    "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
    "print(json.dumps(out_eval, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some samples of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  The French population numbered about 75,000 and was heavily concentrated along the St. Lawrence River valley, with some also in Acadia (present-day New Brunswick and parts of Nova Scotia, including Île Royale (present-day Cape Breton Island)). Fewer lived in New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries. French fur traders and trappers traveled throughout the St. Lawrence and Mississippi watersheds, did business with local tribes, and often married Indian women. Traders married daughters of chiefs, creating high-ranking unions.\n",
      "\n",
      "Question:  What was French population in South America?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  Specialty pharmacies supply high cost injectable, oral, infused, or inhaled medications that are used for chronic and complex disease states such as cancer, hepatitis, and rheumatoid arthritis. Unlike a traditional community pharmacy where prescriptions for any common medication can be brought in and filled, specialty pharmacies carry novel medications that need to be properly stored, administered, carefully monitored, and clinically managed. In addition to supplying these drugs, specialty pharmacies also provide lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs. It is currently the fastest growing sector of the pharmaceutical industry with 19 of 28 newly FDA approved medications in 2013 being specialty drugs.\n",
      "\n",
      "Question:  What is the fastest growing area in the pharmaceutical industry?\n",
      "\n",
      "Answer:  specialty pharmacies\n",
      "\n",
      "Predicted answer:  specialty <unk>\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  The Rhine-Meuse Delta is a tidal delta, shaped not only by the sedimentation of the rivers, but also by tidal currents. This meant that high tide formed a serious risk because strong tidal currents could tear huge areas of land into the sea. Before the construction of the Delta Works, tidal influence was palpable up to Nijmegen, and even today, after the regulatory action of the Delta Works, the tide acts far inland. At the Waal, for example, the most landward tidal influence can be detected between Brakel and Zaltbommel.\n",
      "\n",
      "Question:  What produces a lot of sedimentation that flows into the surrounding rivers?\n",
      "\n",
      "Impossible to answer\n",
      "\n",
      "Predicted impossbile to answer\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  Many major classes of organic molecules in living organisms, such as proteins, nucleic acids, carbohydrates, and fats, contain oxygen, as do the major inorganic compounds that are constituents of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as it is a part of water, the major constituent of lifeforms. Oxygen is used in cellular respiration and released by photosynthesis, which uses the energy of sunlight to produce oxygen from water. It is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (O\n",
      "3), strongly absorbs UVB radiation and consequently the high-altitude ozone layer helps protect the biosphere from ultraviolet radiation, but is a pollutant near the surface where it is a by-product of smog. At even higher low earth orbit altitudes, sufficient atomic oxygen is present to cause erosion for spacecraft.\n",
      "\n",
      "Question:  Photosynthesis uses which energy to for oxygen from water?\n",
      "\n",
      "Answer:  sunlight\n",
      "\n",
      "Predicted answer:  sunlight\n",
      "\n",
      "//////////////////// \n",
      "\n",
      "Context:  According to Sheldon Ungar's comparison with global warming, the actors in the ozone depletion case had a better understanding of scientific ignorance and uncertainties. The ozone case communicated to lay persons \"with easy-to-understand bridging metaphors derived from the popular culture\" and related to \"immediate risks with everyday relevance\", while the public opinion on climate change sees no imminent danger. The stepwise mitigation of the ozone layer challenge was based as well on successfully reducing regional burden sharing conflicts. In case of the IPCC conclusions and the failure of the Kyoto Protocol, varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions remain an unsolved problem. In the UK, a report for a House of Lords committee asked to urge the IPCC to involve better assessments of costs and benefits of climate change but the Stern Review ordered by the UK government made a stronger argument in favor to combat human-made climate change.\n",
      "\n",
      "Question:  What remain unsolved problems with the Kyoto Protocol?\n",
      "\n",
      "Answer:  varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions\n",
      "\n",
      "Predicted answer:  varying regional <unk> analysis and <unk> conflicts with regard to the distribution of emission reductions\n",
      "\n",
      "//////////////////// \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    choice = np.random.choice(list(my_preds))\n",
    "    row = test_df[test_df.id == choice].iloc[0]\n",
    "    print(\"Context: \", str(row.context))\n",
    "    print()\n",
    "    print(\"Question: \", str(row.content))\n",
    "    print()\n",
    "    if row.is_impossible:\n",
    "        print(\"Impossible to answer\")\n",
    "    else:\n",
    "        print(\"Answer: \", row.answer)\n",
    "    print()\n",
    "    if my_preds[choice]:\n",
    "        print(\"Predicted answer: \", my_preds[choice])\n",
    "    else:\n",
    "        print(\"Predicted impossbile to answer\")\n",
    "    print(\"\\n//////////////////// \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
